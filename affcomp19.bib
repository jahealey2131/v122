@Proceedings{AFFCOMP2019,
    booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
    name = {Workshop on Artificial Intelligence in Affective Computing},
    shortname = {AFFCOMP},
    editor = {William Hsu},
    volume = {122},
    year = {2019},
    published = {2020-11-09},
    start = {2019-08-10},
    end = {2019-08-10},
    url = {http://kdd.cs.ksu.edu/Workshops/IJCAI-2019-AffComp/submissions.html},
    address = {Macao, China},
    shortname = {AFFCOMP}
}

@InProceedings{healey19,
    title = {Sequential Dependence and Non-linearity in Affective Responses: a Skin Conductance Example},
    author = {Jennifer Healey},
    pages = {1-8},
    abstract = {Individual affective responses frequently vary from the mean and often exhibit non-linear and time and sequence dependent properties.  This paper examines the extent to which commonly made assumptions of linearity and sequential independence are valid using skin conductance responses to an acoustic stimulus as an example.  We present 19 sessions of skin conductance traces where participants respond to five 50 millisecond acoustic bursts designed to elicit a startle.  We show the data from the perspective of an online algorithm: individual responses, non-linear and dependent on prior events.  We show that the coefficient of variation depends on sequence position and that these are large at 65\%, 97\%, 110\%, and 100\%.  We discuss the risk of making inferences on single impressions.}
}

@InProceedings{lee19,
    title = {Sequential Dependence and Non-linearity in Affective Responses: a Skin Conductance Example},
    author = {Mihee Lee and Ognjen Rudovic and Vladimir Pavlovic and Maja Pantic},
    pages = {9-27},
    abstract = {Detecting facial action unit (AU) activations is one of the key steps in automatic recognition of facial expressions of human emotion and cognitive states. While there are different approaches proposed for this task, most of these are trained only for a specific (sub)set of AUs. As such, they cannot easily adapt to the task of detection of new AUs which are not initially used to train the target models. In this paper, we propose a deep learning approach for facial AU detection that can adapt to a new AU and/or target subject by leveraging only a few labeled samples from the new task (either an AU or subject). We use the notion of the model-agnostic meta-learning, originally proposed for the general image recognition/detection tasks, to design our deep learning models for AU detection. Specifically, each subject and/or AU is treated as a new learning task and the model learns to adapt based on the knowledge of the previously seen tasks. We show on two benchmark datasets (BP4D and DISFA) for facial AU detection that the proposed approach can easily be adapted to new tasks. By using as few as one or five labeled examples from the target task, our approach achieves large improvements over the baseline (non-adapted) deep models.}
}

@InProceedings{niibori19,
    title = {Measuring Two-People Communication from Omnidirectional Video},
    author = {Yui Niibori and Shigang Li},
    pages = {28-35},
    abstract = {In this paper we propose a method of measuring the communication between two people by analyzing their heads' information: head pose, gaze vectors and facial action units. Assuming two people are sitting around a table, an omnidirectional camera is used to observe the two people simultaneously.Next, the visual cues of the heads of the two people, including head pose, gaze vectors and facial action units, are extracted using a popular facial behavior analysis toolkit, OpenFace. Then,  a LSTM (Long Short Term Memory) neural network is used to learn measuring the communication between the two people from the temporal sequence of the extracted head information. The preliminary experimental results show the effectiveness of the proposed method.}
}

@InProceedings{sinha19,
    title = {Persuasion: What Jane Austin Would Have Written},
    author = {Moumita Sinha and Jennifer Healey and Faran Ahmad and Varun Gupta and Niloy Ganguly},
    pages = {36-43},
    abstract = {This paper presents preliminary results for developing an online "persuasion score" that will enable digital marketing content authors to compose and edit materials with better persuasive capability.  Inspired by initial insights with digital marketing professionals and research on the foundations of persuasion: pathos, ethos and logos, we extracted features from a data set of over three million consumer reactions to email marketing campaigns covering a three month period.  We report on the most significant features of the content, including image position and text readability as well as the most salient customer features such as time since registration and time since last opened email from the same marketing brand.}
}

@InProceedings{tripathi19,
    title = {Learning Discriminative Features using Center Loss and Reconstruction as Regularizer for Speech Emotion Recognition},
    author = {Suraj Tripathi and Abhiram Ramesh and Abhay Kumar and Chirag Singh and Promod Yenigalla},
    pages = {44-53},
    abstract = {This paper proposes a Convolutional Neural Network (CNN) inspired by Multitask Learning (MTL) and based on speech features trained under the joint supervision of softmax loss and center loss, a powerful metric learning strategy, for the recognition of emotion in speech. Speech features such as Spectrograms and Mel-frequency Cepstral Coefficients (MFCCs) help retain emotion related low-level characteristics in speech. We experimented with several Deep Neural Network (DNN) architectures that take in speech features as input and trained them under both softmax and center loss, which resulted in highly discriminative features ideal for Speech Emotion Recognition (SER). Our networks also employ a regularizing effect by simultaneously performing the auxiliary task of reconstructing the input speech features. This sharing of representations among related tasks enables our network to better generalize the original task of SER. Some of our proposed networks contain far fewer parameters when compared to state-of-the-art architectures. We used the University of Southern Californiaâ€™s Interactive Emotional Motion Capture (USC-IEMOCAP) database in this work. Our best performing model achieves a 3.1\% improvement in overall accuracy and a 5.3\% improvement in class accuracy when compared to existing state-of-the-art methods.}
}

